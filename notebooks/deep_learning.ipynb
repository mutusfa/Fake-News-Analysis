{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "from jjuoda_dl4 import utils\n",
    "from jjuoda_dl4.utils import BASE_DATA_DIR, BASE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nela_gt_2018_articles_df = pd.read_csv(\n",
    "    BASE_DATA_DIR / \"interim/nela-gt-2018-articles.csv\", index_col=0\n",
    ")\n",
    "nela_gt_2018_scores_df = pd.read_csv(\n",
    "    BASE_DATA_DIR / \"interim/nela-gt-2018-scores.csv\", index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have:\n",
      "33 reliable sources in train\n",
      "13 unreliable sources in train\n",
      "For a total of 133215+55210=188425 articles in train\n",
      "3 reliable sources in val\n",
      "2 unreliable sources in val\n",
      "For a total of 6091+5937=12028 articles in val\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nela_gt_2018_articles_df = utils.split_dataframe(\n",
    "    nela_gt_2018_articles_df, nela_gt_2018_scores_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e7c8ff97c94e178442a7852d68f66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/697538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nela_gt_2018_articles_df = utils._make_dataframe(nela_gt_2018_articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoNLP from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataModules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried using hugging face api, my machine runs out of ram, so we'll use pytorch lightning\n",
    "\n",
    "\n",
    "class AutoNLPNELADataset(Dataset):\n",
    "    \"\"\"Recreates processeing I did for AutoNLP on nela gt 2018 data.\"\"\"\n",
    "\n",
    "    def __init__(self, articles_df, tokenizer, root_dir=BASE_DATA_DIR):\n",
    "        self.articles_df = articles_df\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        article = self.articles_df.iloc[index]\n",
    "        with open(self.root_dir / article.path, \"r\") as f:\n",
    "            text = f.read().replace(\"\\n\", \" \")\n",
    "        text = \"<TITLE> \" + article.title + \" </TITLE> \" + text\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"longest_first\",\n",
    "        )\n",
    "        return {\n",
    "            \"model_inputs\": inputs,\n",
    "            \"source_score\": article.source_score,\n",
    "            \"is_fake\": article.source_score < 0,\n",
    "        }\n",
    "\n",
    "\n",
    "class AutoNLPNelaDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        articles_df,\n",
    "        tokenizer,\n",
    "        root_dir=BASE_DATA_DIR,\n",
    "        batch_size=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.articles_df = articles_df\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_dataset = AutoNLPNELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"train\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "        self.val_dataset = AutoNLPNELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"val\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "        self.test_dataset = AutoNLPNELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"test\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "        self.pred_dataset = AutoNLPNELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"pred\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "    def pred_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.pred_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just hope to get quick results that I can look at and refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e16946f2977477a8bc13f3e3c054e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"mutusfa/autonlp-Fake_News_Analysis-528914957\",\n",
    "    \"mutusfa/autonlp-Fake_News_Analysis-528914958\",\n",
    "    \"mutusfa/autonlp-Fake_News_Analysis-528914959\",\n",
    "    \"mutusfa/autonlp-Fake_News_Analysis-528914960\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randn(1000)\n",
    "x = np.random.randint(0, 10, size=(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def plot_predictions(articles_df, model_name):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, use_auth_token=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    data_module = AutoNLPNelaDataModule(articles_df, tokenizer, batch_size=64)\n",
    "    data_module.prepare_data()\n",
    "    dataloaders = [data_module.pred_dataloader()]\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    source_scores = []\n",
    "    with torch.no_grad():\n",
    "        for dataloader in dataloaders:\n",
    "            for batch in dataloader:\n",
    "                # I'm not sure why do I get 32 * 1 * 512 tensors\n",
    "                for k in batch[\"model_inputs\"]:\n",
    "                    batch[\"model_inputs\"][k] = (\n",
    "                        batch[\"model_inputs\"][k].squeeze(1).to(device)\n",
    "                    )\n",
    "                preds.extend(\n",
    "                    sigmoid(model(**batch[\"model_inputs\"]).logits[:, 1].cpu().numpy())\n",
    "                )\n",
    "                source_scores.extend(batch[\"source_score\"].numpy())\n",
    "\n",
    "    plt.figure()\n",
    "    ax = sns.regplot(\n",
    "        x=source_scores,\n",
    "        y=preds,\n",
    "        x_jitter=0.1,\n",
    "        line_kws={\"color\": \"#859900\"},\n",
    "        scatter_kws={\"alpha\": 0.5},\n",
    "    )\n",
    "    plt.suptitle(model_name)\n",
    "    plt.ylabel(\"Predicted probability of being fake\")\n",
    "    plt.xlabel(\"Source score\")\n",
    "    plt.savefig(\n",
    "        BASE_DATA_DIR / f\"processed/{model_name.replace('/', '_')}_reg_predictions.png\"\n",
    "    )\n",
    "\n",
    "    return preds, source_scores\n",
    "\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     preds, scores = plot_predictions(\n",
    "#         nela_gt_2018_articles_df[nela_gt_2018_articles_df.split == \"pred\"].sample(\n",
    "#             n=1000, random_state=42\n",
    "#         ),\n",
    "#         model_name,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models achieved much better auc than knn models (all above .95 versus .86 I had with knn) on their validation set. The only problem - training set was quite imbalanced and I have changed both training set and validation set, so they are not directly comparable.\n",
    "\n",
    "Looking at the data both models didn't see, we see interesting differences.\n",
    "1. KNNs predict most news as having high probability of being fake news, while AutoNLP models predict most news as having low probability of being fake news.\n",
    "1. It also seems that AutoNLP models generalize pretty badly - some can't distinguish between sources with -1 score and sources with 1 score. I guess I have to check if I can distinguish that, but knns seem to be able to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try tuning a few models and see if I can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NELADataset(Dataset):\n",
    "    \"\"\"Recreates processeing I did for AutoNLP on nela gt 2018 data.\"\"\"\n",
    "\n",
    "    def __init__(self, articles_df, tokenizer, root_dir=BASE_DATA_DIR):\n",
    "        self.articles_df = articles_df\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        article = self.articles_df.iloc[index]\n",
    "        if \"text\" in article.keys():\n",
    "            text = article[\"text\"]\n",
    "        else:\n",
    "            with open(self.root_dir / article.path, \"r\") as f:\n",
    "                text = f.read()\n",
    "        inputs = self.tokenizer(\n",
    "            article.title,\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"longest_first\",\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = torch.tensor([article.source_score < 0])\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class NelaDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        articles_df,\n",
    "        tokenizer,\n",
    "        root_dir=BASE_DATA_DIR,\n",
    "        batch_size=16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.articles_df = articles_df\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_dataset = NELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"train\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "        self.val_dataset = NELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"val\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "        self.test_dataset = NELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"test\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "        self.pred_dataset = NELADataset(\n",
    "            self.articles_df[self.articles_df.split == \"pred\"],\n",
    "            self.tokenizer,\n",
    "            self.root_dir,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "    def pred_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.pred_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nela_gt_2018_data_module = NelaDataModule(nela_gt_2018_articles_df, tokenizer)\n",
    "nela_gt_2018_data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NelaModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, **model_inputs):\n",
    "        return self.model(**model_inputs)\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        loss, inference = self.model(**batch)\n",
    "        return loss, inference\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, inference = self._shared_step(batch, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "nela_model = NelaModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/julius/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:120: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model | DistilBertForSequenceClassification | 67.0 M\n",
      "--------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/11777 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12819/1599911783.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  inputs[\"labels\"] = torch.tensor([article.source_score < 0])\n",
      "/tmp/ipykernel_12819/1599911783.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  inputs[\"labels\"] = torch.tensor([article.source_score < 0])\n",
      "/tmp/ipykernel_12819/1599911783.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  inputs[\"labels\"] = torch.tensor([article.source_score < 0])\n",
      "/tmp/ipykernel_12819/1599911783.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  inputs[\"labels\"] = torch.tensor([article.source_score < 0])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 5.80 GiB total capacity; 4.38 GiB already allocated; 192.81 MiB free; 4.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000018?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000018?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(nela_model, nela_gt_2018_data_module)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=734'>735</a>\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=735'>736</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=736'>737</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=737'>738</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=738'>739</a>\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=739'>740</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=740'>741</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=741'>742</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=674'>675</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=675'>676</a>\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=676'>677</a>\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=681'>682</a>\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=682'>683</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=683'>684</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=684'>685</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=685'>686</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=686'>687</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=774'>775</a>\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=775'>776</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=776'>777</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=778'>779</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=779'>780</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1195'>1196</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1197'>1198</a>\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1198'>1199</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1200'>1201</a>\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1201'>1202</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1276'>1277</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1277'>1278</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1278'>1279</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=200'>201</a>\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=201'>202</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1286'>1287</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1287'>1288</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1288'>1289</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1316'>1317</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1317'>1318</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=230'>231</a>\u001b[0m data_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=232'>233</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=233'>234</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(data_fetcher)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=235'>236</a>\u001b[0m     \u001b[39m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=236'>237</a>\u001b[0m     \u001b[39m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=237'>238</a>\u001b[0m     \u001b[39m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=238'>239</a>\u001b[0m     \u001b[39m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py?line=239'>240</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:193\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=189'>190</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=191'>192</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=192'>193</a>\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=194'>195</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=196'>197</a>\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=197'>198</a>\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=85'>86</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=86'>87</a>\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=87'>88</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=88'>89</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py?line=89'>90</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=142'>143</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=143'>144</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=144'>145</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:215\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=213'>214</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=214'>215</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=215'>216</a>\u001b[0m         batch,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=216'>217</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=217'>218</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=218'>219</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=219'>220</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=220'>221</a>\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=221'>222</a>\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=222'>223</a>\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=223'>224</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:266\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=258'>259</a>\u001b[0m         closure()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=260'>261</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=261'>262</a>\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=262'>263</a>\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=263'>264</a>\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=264'>265</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=265'>266</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=267'>268</a>\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=269'>270</a>\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=270'>271</a>\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=271'>272</a>\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=272'>273</a>\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:378\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=374'>375</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=376'>377</a>\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=377'>378</a>\u001b[0m lightning_module\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=378'>379</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=379'>380</a>\u001b[0m     batch_idx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=380'>381</a>\u001b[0m     optimizer,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=381'>382</a>\u001b[0m     opt_idx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=382'>383</a>\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=383'>384</a>\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_device_type \u001b[39m==\u001b[39;49m DeviceType\u001b[39m.\u001b[39;49mTPU \u001b[39mand\u001b[39;49;00m _TPU_AVAILABLE),\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=384'>385</a>\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mand\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=385'>386</a>\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=386'>387</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=388'>389</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:1652\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1570'>1571</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1571'>1572</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1572'>1573</a>\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1579'>1580</a>\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1580'>1581</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1581'>1582</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1582'>1583</a>\u001b[0m \u001b[39m    Override this method to adjust the default way the\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1583'>1584</a>\u001b[0m \u001b[39m    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1649'>1650</a>\u001b[0m \n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1650'>1651</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py?line=1651'>1652</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:164\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py?line=161'>162</a>\u001b[0m \u001b[39massert\u001b[39;00m trainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py?line=162'>163</a>\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(profiler_action):\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py?line=163'>164</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:336\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=325'>326</a>\u001b[0m \u001b[39m\"\"\"performs the actual optimizer step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=326'>327</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=327'>328</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=332'>333</a>\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=333'>334</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=334'>335</a>\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=335'>336</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:163\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=160'>161</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=161'>162</a>\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=162'>163</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/adam.py?line=89'>90</a>\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/adam.py?line=90'>91</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m---> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/adam.py?line=91'>92</a>\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/adam.py?line=93'>94</a>\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m     <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/optim/adam.py?line=94'>95</a>\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:148\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=134'>135</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=135'>136</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=136'>137</a>\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=139'>140</a>\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=140'>141</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=141'>142</a>\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=142'>143</a>\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=143'>144</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=144'>145</a>\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=145'>146</a>\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=146'>147</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=147'>148</a>\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=148'>149</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py?line=149'>150</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:160\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=158'>159</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=159'>160</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=160'>161</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:142\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=139'>140</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=140'>141</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_and_backward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=141'>142</a>\u001b[0m         step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=143'>144</a>\u001b[0m         \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=144'>145</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=145'>146</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=146'>147</a>\u001b[0m             )\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:435\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=432'>433</a>\u001b[0m lightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=433'>434</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=434'>435</a>\u001b[0m     training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mtraining_step(step_kwargs)\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=435'>436</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py?line=437'>438</a>\u001b[0m \u001b[39mdel\u001b[39;00m step_kwargs\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:216\u001b[0m, in \u001b[0;36mAccelerator.training_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=210'>211</a>\u001b[0m \u001b[39m\"\"\"The actual training step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=211'>212</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=212'>213</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=213'>214</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=214'>215</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py?line=215'>216</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:213\u001b[0m, in \u001b[0;36mTrainingTypePlugin.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=211'>212</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py?line=212'>213</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb Cell 18'\u001b[0m in \u001b[0;36mNelaModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000017?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000017?line=13'>14</a>\u001b[0m     loss, inference \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shared_step(batch, batch_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000017?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[1;32m/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb Cell 18'\u001b[0m in \u001b[0;36mNelaModel._shared_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000017?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_shared_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000017?line=9'>10</a>\u001b[0m     loss, inference \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/julius/lp/Turing/jjuoda-DL.4/notebooks/deep_learning.ipynb#ch0000017?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, inference\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:727\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=718'>719</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=719'>720</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=720'>721</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=721'>722</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=722'>723</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=723'>724</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=724'>725</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=726'>727</a>\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=727'>728</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=728'>729</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=729'>730</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=730'>731</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=731'>732</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=732'>733</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=733'>734</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=734'>735</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=735'>736</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=736'>737</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=546'>547</a>\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=547'>548</a>\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=548'>549</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=549'>550</a>\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=550'>551</a>\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=551'>552</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=552'>553</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=553'>554</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=554'>555</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=555'>556</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:327\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=323'>324</a>\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=324'>325</a>\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=326'>327</a>\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=327'>328</a>\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=328'>329</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=329'>330</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=331'>332</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:271\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=260'>261</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=261'>262</a>\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=262'>263</a>\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=267'>268</a>\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=268'>269</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=269'>270</a>\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=270'>271</a>\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=271'>272</a>\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=272'>273</a>\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=273'>274</a>\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=274'>275</a>\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=275'>276</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=276'>277</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=277'>278</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=278'>279</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=279'>280</a>\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:205\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=201'>202</a>\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=203'>204</a>\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=204'>205</a>\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=205'>206</a>\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.conda/envs/jjuoda-DL.4/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=206'>207</a>\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(mask, \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 5.80 GiB total capacity; 4.38 GiB already allocated; 192.81 MiB free; 4.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=3, gpus=1)\n",
    "trainer.fit(nela_model, nela_gt_2018_data_module)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a7d5478afe2248c49abfa1219649289765da77ad68068bfc5af38a5747c85f1"
  },
  "kernelspec": {
   "display_name": "jjuoda-DL.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
