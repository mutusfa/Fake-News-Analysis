{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LuATy2Dt9rUM",
        "outputId": "3280bbcc-d883-445a-e623-a218918becdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/mutusfa/Fake-News-Analysis scikit-learn-intelex huggingface_hub pytorch_lightning transformers"
      ],
      "metadata": {
        "id": "4vuljgCX4-eo",
        "outputId": "d783c01d-ee83-44b8-dc00-d75270cedfc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/mutusfa/Fake-News-Analysis\n",
            "  Cloning https://github.com/mutusfa/Fake-News-Analysis to /tmp/pip-req-build-bqea779p\n",
            "  Running command git clone -q https://github.com/mutusfa/Fake-News-Analysis /tmp/pip-req-build-bqea779p\n",
            "Collecting scikit-learn-intelex\n",
            "  Downloading scikit_learn_intelex-2021.5.3-py37-none-manylinux1_x86_64.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-intelex) (1.0.2)\n",
            "Collecting daal4py==2021.5.3\n",
            "  Downloading daal4py-2021.5.3-py37-none-manylinux1_x86_64.whl (22.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.5 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting daal==2021.5.3\n",
            "  Downloading daal-2021.5.3-py2.py3-none-manylinux1_x86_64.whl (284.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 284.3 MB 2.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from daal4py==2021.5.3->scikit-learn-intelex) (1.19.5)\n",
            "Collecting tbb==2021.*\n",
            "  Downloading tbb-2021.5.1-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.7)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.5 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 81.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |███████████████████████████████▉| 890 kB 56.1 MB/s eta 0:00:01"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Monkey patch for runtime in colab\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "imoprt jjuoda_dl4.utils\n",
        "\n",
        "# the function that reads articles will default to old path but whatever\n",
        "jjuoda_dl4.utils.BASE_DATA_DIR = Path(\"/content/drive/MyDrive/Fake News Analysis/Data\")\n",
        "jjuoda_dl4.utils.BASE_MODEL_DIR = Path(\"/content/drive/MyDrive/Fake News Analysis/Models\")"
      ],
      "metadata": {
        "id": "QXJ-kRt89v5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoSp_U0H28V4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "\n",
        "patch_sklearn()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.functional import auroc\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "from jjuoda_dl4 import utils\n",
        "from jjuoda_dl4.utils import BASE_DATA_DIR, BASE_MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20EOiwzK28V-"
      },
      "outputs": [],
      "source": [
        "nela_gt_2018_articles_df = pd.read_csv(\n",
        "    BASE_DATA_DIR / \"interim/nela-gt-2018-articles.csv\", index_col=0\n",
        ")\n",
        "nela_gt_2018_scores_df = pd.read_csv(\n",
        "    BASE_DATA_DIR / \"interim/nela-gt-2018-scores.csv\", index_col=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JgWOMNJ28V_"
      },
      "outputs": [],
      "source": [
        "nela_gt_2018_articles_df = utils.split_dataframe(\n",
        "    nela_gt_2018_articles_df, nela_gt_2018_scores_df\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQhTQgMR28V_"
      },
      "outputs": [],
      "source": [
        "nela_gt_2018_articles_df = utils._make_dataframe(nela_gt_2018_articles_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Z5XTcP28WA"
      },
      "source": [
        "### AutoNLP from HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su-s2Y6O28WB"
      },
      "source": [
        "#### DataModules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH-Z_dr828WB"
      },
      "outputs": [],
      "source": [
        "# I tried using hugging face api, my machine runs out of ram, so we'll use pytorch lightning\n",
        "\n",
        "\n",
        "class AutoNLPNELADataset(Dataset):\n",
        "    \"\"\"Recreates processeing I did for AutoNLP on nela gt 2018 data.\"\"\"\n",
        "\n",
        "    def __init__(self, articles_df, tokenizer, root_dir=BASE_DATA_DIR):\n",
        "        self.articles_df = articles_df\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles_df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        article = self.articles_df.iloc[index]\n",
        "        with open(self.root_dir / article.path, \"r\") as f:\n",
        "            text = f.read().replace(\"\\n\", \" \")\n",
        "        text = \"<TITLE> \" + article.title + \" </TITLE> \" + text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=\"longest_first\",\n",
        "        )\n",
        "        return {\n",
        "            \"model_inputs\": inputs,\n",
        "            \"source_score\": article.source_score,\n",
        "            \"is_fake\": article.source_score < 0,\n",
        "        }\n",
        "\n",
        "\n",
        "class AutoNLPNelaDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        articles_df,\n",
        "        tokenizer,\n",
        "        root_dir=BASE_DATA_DIR,\n",
        "        batch_size=32,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.articles_df = articles_df\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.train_dataset = AutoNLPNELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"train\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "        self.val_dataset = AutoNLPNELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"val\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "        self.test_dataset = AutoNLPNELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"test\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "        self.pred_dataset = AutoNLPNELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"pred\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
        "        )\n",
        "\n",
        "    def pred_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.pred_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SaWmuqZ28WE"
      },
      "source": [
        "I just hope to get quick results that I can look at and refine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC8mVOIG28WE"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShU4Y4U28WF"
      },
      "outputs": [],
      "source": [
        "model_names = [\n",
        "    \"mutusfa/autonlp-Fake_News_Analysis-528914957\",\n",
        "    \"mutusfa/autonlp-Fake_News_Analysis-528914958\",\n",
        "    \"mutusfa/autonlp-Fake_News_Analysis-528914959\",\n",
        "    \"mutusfa/autonlp-Fake_News_Analysis-528914960\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNdb_3PG28WG"
      },
      "outputs": [],
      "source": [
        "y = np.random.randn(1000)\n",
        "x = np.random.randint(0, 10, size=(1000,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2iJiiDw28WG"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def plot_predictions(articles_df, model_name):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, use_auth_token=True\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    data_module = AutoNLPNelaDataModule(articles_df, tokenizer, batch_size=64)\n",
        "    data_module.prepare_data()\n",
        "    dataloaders = [data_module.pred_dataloader()]\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    source_scores = []\n",
        "    with torch.no_grad():\n",
        "        for dataloader in dataloaders:\n",
        "            for batch in dataloader:\n",
        "                # I'm not sure why do I get 32 * 1 * 512 tensors\n",
        "                for k in batch[\"model_inputs\"]:\n",
        "                    batch[\"model_inputs\"][k] = (\n",
        "                        batch[\"model_inputs\"][k].squeeze(1).to(device)\n",
        "                    )\n",
        "                preds.extend(\n",
        "                    sigmoid(model(**batch[\"model_inputs\"]).logits[:, 1].cpu().numpy())\n",
        "                )\n",
        "                source_scores.extend(batch[\"source_score\"].numpy())\n",
        "\n",
        "    plt.figure()\n",
        "    ax = sns.regplot(\n",
        "        x=source_scores,\n",
        "        y=preds,\n",
        "        x_jitter=0.1,\n",
        "        line_kws={\"color\": \"#859900\"},\n",
        "        scatter_kws={\"alpha\": 0.5},\n",
        "    )\n",
        "    plt.suptitle(model_name)\n",
        "    plt.ylabel(\"Predicted probability of being fake\")\n",
        "    plt.xlabel(\"Source score\")\n",
        "    plt.savefig(\n",
        "        BASE_DATA_DIR / f\"processed/{model_name.replace('/', '_')}_reg_predictions.png\"\n",
        "    )\n",
        "\n",
        "    return preds, source_scores\n",
        "\n",
        "\n",
        "# for model_name in model_names:\n",
        "#     preds, scores = plot_predictions(\n",
        "#         nela_gt_2018_articles_df[nela_gt_2018_articles_df.split == \"pred\"].sample(\n",
        "#             n=1000, random_state=42\n",
        "#         ),\n",
        "#         model_name,\n",
        "#     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xPZUg5r28WH"
      },
      "source": [
        "These models achieved much better auc than knn models (all above .95 versus .86 I had with knn) on their validation set. The only problem - training set was quite imbalanced and I have changed both training set and validation set, so they are not directly comparable.\n",
        "\n",
        "Looking at the data both models didn't see, we see interesting differences.\n",
        "1. KNNs predict most news as having high probability of being fake news, while AutoNLP models predict most news as having low probability of being fake news.\n",
        "1. It also seems that AutoNLP models generalize pretty badly - some can't distinguish between sources with -1 score and sources with 1 score. I guess I have to check if I can distinguish that, but knns seem to be able to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq2UQZCw28WH"
      },
      "source": [
        "Let's try tuning a few models and see if I can get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMXWfhQW28WH"
      },
      "source": [
        "### My work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAFa3Odq28WH"
      },
      "outputs": [],
      "source": [
        "class NELADataset(Dataset):\n",
        "    \"\"\"Recreates processeing I did for AutoNLP on nela gt 2018 data.\"\"\"\n",
        "\n",
        "    def __init__(self, articles_df, tokenizer, root_dir=BASE_DATA_DIR):\n",
        "        self.articles_df = articles_df\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles_df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        article = self.articles_df.iloc[index]\n",
        "        if \"text\" in article.keys():\n",
        "            text = article[\"text\"]\n",
        "        else:\n",
        "            with open(self.root_dir / article.path, \"r\") as f:\n",
        "                text = f.read()\n",
        "        inputs = self.tokenizer(\n",
        "            article.title,\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=\"longest_first\",\n",
        "        )\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs[\"labels\"] = torch.tensor([article.source_score < 0], dtype=int)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "class NelaDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        articles_df,\n",
        "        tokenizer,\n",
        "        root_dir=BASE_DATA_DIR,\n",
        "        batch_size=16,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.articles_df = articles_df\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.train_dataset = NELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"train\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "        self.val_dataset = NELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"val\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "        self.test_dataset = NELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"test\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "        self.pred_dataset = NELADataset(\n",
        "            self.articles_df[self.articles_df.split == \"pred\"],\n",
        "            self.tokenizer,\n",
        "            self.root_dir,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
        "        )\n",
        "\n",
        "    def pred_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.pred_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2fSZ4JL28WI"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\n",
        "    \"distilbert-base-uncased\"\n",
        ")\n",
        "model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoq4QylY28WI"
      },
      "outputs": [],
      "source": [
        "nela_gt_2018_data_module = NelaDataModule(nela_gt_2018_articles_df, tokenizer)\n",
        "nela_gt_2018_data_module.prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkzRQD4U28WJ"
      },
      "outputs": [],
      "source": [
        "class NelaModel(pl.LightningModule):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def backbone_grad(self, value):\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = value\n",
        "        for param in self.model.pre_classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "        for param in self.model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    def forward(self, **model_inputs):\n",
        "        return self.model(**model_inputs)\n",
        "\n",
        "    def _shared_step(self, batch, batch_idx):\n",
        "        return self.model(**batch)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        results = self._shared_step(batch, batch_idx)\n",
        "        self.log(\"train_loss\", results.loss.detach())\n",
        "        return results\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        results = self._shared_step(batch, batch_idx)\n",
        "        self.log(\"val_loss\", results.loss.detach())\n",
        "        self.log(\"val_auc\", auroc(torch.sigmoid(results.logits.detach()[:, 1]), batch[\"labels\"], num_classes=2))\n",
        "        return results\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            params=[p for p in self.parameters() if p.requires_grad],\n",
        "            lr=self.learning_rate,\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=3\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"val_loss\",\n",
        "        }\n",
        "\n",
        "\n",
        "nela_model = NelaModel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzmf7Vrt28WJ"
      },
      "outputs": [],
      "source": [
        "nela_model.backbone_grad(False)\n",
        "nela_model.learning_rate = 1e-3\n",
        "trainer = pl.Trainer(\n",
        "    min_epochs=3,\n",
        "    gpus=1,                     \n",
        "    callbacks=[\n",
        "        pl.callbacks.EarlyStopping(\n",
        "            patience=5, min_delta=0.0001, monitor=\"val_loss/dataloader_idx_1\"\n",
        "        ),\n",
        "        pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=False),\n",
        "        pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
        "            BASE_DATA_DIR / \"best\",\n",
        "            filename='{epoch}-{val_loss:.2f}-{val_auc:.2f}',\n",
        "            monitor=\"val_loss\",\n",
        "        )\n",
        "    ],\n",
        "    default_root_dir=BASE_MODEL_DIR,\n",
        ")\n",
        "trainer.fit(nela_model, nela_gt_2018_data_module)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nela_model.backbone_grad(True)\n",
        "nela_model.learning_rate = 1e-5\n",
        "trainer = pl.Trainer(\n",
        "    min_epochs=3,\n",
        "    gpus=1,                     \n",
        "    callbacks=[\n",
        "        pl.callbacks.EarlyStopping(\n",
        "            patience=5, min_delta=0.0001, monitor=\"val_loss/dataloader_idx_1\"\n",
        "        ),\n",
        "        pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=False),\n",
        "        pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
        "            BASE_DATA_DIR / \"best\",\n",
        "            filename='{epoch}-{val_loss:.2f}-{val_auc:.2f}',\n",
        "            monitor=\"val_loss\",\n",
        "        )\n",
        "    ],\n",
        "    default_root_dir=BASE_MODEL_DIR,\n",
        ")\n",
        "trainer.fit(nela_model, nela_gt_2018_data_module)"
      ],
      "metadata": {
        "id": "RH6Z5eD39WuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "5a7d5478afe2248c49abfa1219649289765da77ad68068bfc5af38a5747c85f1"
    },
    "kernelspec": {
      "display_name": "jjuoda-DL.4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "deep_learning.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}